{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    def __init__(self, environment_name, episodes, epsilon = 0.2, alpha=0.2, gamma=0.95):\n",
    "        self.env = gym.make(environment_name)\n",
    "        self.episodes = episodes\n",
    "        self.episode_data = 500\n",
    "        self.param = alpha\n",
    "        self.ep_rewards_table = {'ep': [], 'avg_'+str(self.param): [], 'min_'+str(self.param): [], 'max_'+str(self.param): [], 'std_'+str(self.param): [], 'mid_'+str(self.param): []}\n",
    "        \n",
    "        # Initialize Q\n",
    "        self.space_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50) / 1.]\n",
    "        self.lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50) / 1.]\n",
    "        self.number_bins = 50\n",
    "        self.Q = np.random.randn(self.number_bins, self.number_bins, self.number_bins, self.number_bins, self.action_size)\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def select_e_greedy(self, state, epsilon):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.Q[state])\n",
    "\n",
    "    def discretize_state(self, observation):\n",
    "        pos_index = np.argmin(np.abs(np.linspace(self.lower_bounds[0], self.upper_bounds[0], num=self.number_bins).tolist() - observation[0]))\n",
    "        vel_index = np.argmin(np.abs(np.linspace(self.lower_bounds[1], self.upper_bounds[1], num=self.number_bins).tolist() - observation[1]))\n",
    "        ang_index = np.argmin(np.abs(np.linspace(self.lower_bounds[2], self.upper_bounds[2], num=self.number_bins).tolist() - observation[2]))\n",
    "        ang_vel_index = np.argmin(np.abs(np.linspace(self.lower_bounds[3], self.upper_bounds[3], num=self.number_bins).tolist() - observation[3]))\n",
    "        return pos_index, vel_index, ang_index, ang_vel_index\n",
    "\n",
    "    def test_q(self):\n",
    "        state, info = self.env.reset()\n",
    "        score = 0 \n",
    "        epsilon = 0.0\n",
    "        d_state = self.discretize_state(state)\n",
    "        action = self.select_e_greedy(d_state, epsilon)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        while not (terminated or truncated or score > 500):\n",
    "            state_prime, reward, terminated, truncated, info = self.env.step(action)\n",
    "            d_state_prime = self.discretize_state(state_prime)\n",
    "            action_prime = self.select_e_greedy(d_state_prime, epsilon)\n",
    "            d_state = d_state_prime\n",
    "            action = action_prime\n",
    "            score += reward\n",
    "        self.env.close()\n",
    "        return score\n",
    "\n",
    "    def train(self):\n",
    "        ep_rewards = []\n",
    "        ep_rewards_t = []\n",
    "\n",
    "        for episode in range(1, self.episodes + 1):\n",
    "            state, info = self.env.reset()\n",
    "            score = 0 \n",
    "            d_state = self.discretize_state(state)\n",
    "            #action = self.select_e_greedy(d_state, self.epsilon)\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "\n",
    "            while not (terminated or truncated or score > 500):\n",
    "                \n",
    "                action = self.select_e_greedy(d_state, self.epsilon)\n",
    "\n",
    "                state_prime, reward, terminated, truncated, info = self.env.step(action)\n",
    "                d_state_prime = self.discretize_state(state_prime)        \n",
    "            \n",
    "                self.Q[d_state+(action,)] += self.alpha * (reward + self.gamma * np.max(self.Q[d_state_prime]) - self.Q[d_state+(action,)])\n",
    "                \n",
    "                d_state = d_state_prime\n",
    "                score += reward\n",
    "\n",
    "            t_score = self.test_q()\n",
    "            ep_rewards.append(score)\n",
    "            ep_rewards_t.append(t_score)\n",
    "\n",
    "            if not episode % self.episode_data:\n",
    "                avg_reward = sum(ep_rewards_t[-self.episode_data:]) / len(ep_rewards_t[-self.episode_data:])\n",
    "                self.ep_rewards_table['ep'].append(episode)\n",
    "                self.ep_rewards_table['avg_'+str(self.param)].append(avg_reward)\n",
    "                self.ep_rewards_table['min_'+str(self.param)].append(min(ep_rewards_t[-self.episode_data:]))\n",
    "                self.ep_rewards_table['max_'+str(self.param)].append(max(ep_rewards_t[-self.episode_data:]))\n",
    "                self.ep_rewards_table['std_'+str(self.param)].append(np.std(ep_rewards_t[-self.episode_data:]))\n",
    "                self.ep_rewards_table['mid_'+str(self.param)].append(statistics.median(ep_rewards_t[-self.episode_data:]))\n",
    "                \n",
    "                print(f\"Episode:{episode} avg:{avg_reward} min:{min(ep_rewards_t[-self.episode_data:])} max:{max(ep_rewards_t[-self.episode_data:])} std:{np.std(ep_rewards_t[-self.episode_data:])}\")\n",
    "\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 1\n",
      "Episode:500 avg:21.456 min:8.0 max:172.0 std:15.058952951649726\n",
      "Episode:1000 avg:19.434 min:8.0 max:108.0 std:11.322086556814517\n",
      "Episode:1500 avg:18.26 min:8.0 max:114.0 std:10.851193482746494\n",
      "Episode:2000 avg:20.814 min:8.0 max:147.0 std:14.574203374455841\n",
      "Episode:2500 avg:26.778 min:8.0 max:465.0 std:30.59837766941248\n",
      "Episode:3000 avg:40.644 min:8.0 max:500.0 std:53.19328213223922\n",
      "Episode:3500 avg:56.624 min:8.0 max:500.0 std:80.35784108598239\n",
      "Episode:4000 avg:46.324 min:8.0 max:377.0 std:51.969943467354284\n",
      "Episode:4500 avg:55.178 min:9.0 max:500.0 std:75.07024920699278\n",
      "Episode:5000 avg:89.12 min:8.0 max:500.0 std:121.91451759327107\n",
      "Episode:5500 avg:67.96 min:8.0 max:500.0 std:92.9232930970486\n",
      "Episode:6000 avg:73.866 min:9.0 max:500.0 std:90.11308475465702\n",
      "Episode:6500 avg:76.942 min:9.0 max:500.0 std:94.38901756030731\n",
      "Episode:7000 avg:92.072 min:9.0 max:500.0 std:101.72963587863666\n",
      "Episode:7500 avg:102.512 min:10.0 max:500.0 std:110.3827788017678\n",
      "Episode:8000 avg:91.05 min:9.0 max:500.0 std:100.85982103890528\n",
      "Episode:8500 avg:125.1 min:11.0 max:500.0 std:135.54850792244082\n",
      "Episode:9000 avg:129.49 min:9.0 max:500.0 std:131.78455865540545\n",
      "Episode:9500 avg:99.74 min:9.0 max:500.0 std:100.85165541526821\n",
      "Episode:10000 avg:139.83 min:9.0 max:500.0 std:150.2380015175921\n",
      "Episode:10500 avg:154.448 min:9.0 max:500.0 std:141.5588615947444\n",
      "Episode:11000 avg:126.912 min:9.0 max:500.0 std:124.92468233299614\n",
      "Episode:11500 avg:137.404 min:12.0 max:500.0 std:123.75771807851015\n",
      "Episode:12000 avg:137.994 min:12.0 max:500.0 std:128.78991406162208\n"
     ]
    }
   ],
   "source": [
    "alphas = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4]\n",
    "#alphas = [0.9, 0.8]\n",
    "episodes = 50001\n",
    "episode_data = 500\n",
    "environment_name = \"CartPole-v1\"\n",
    "agents_ql_alpha = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(f\"Alpha = {alpha}\")\n",
    "    agent = Qlearning(environment_name, episodes, epsilon = 0.25, alpha= alpha, gamma= 0.95)\n",
    "    agent.train()\n",
    "    agents_ql_alpha.append(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "for agent in agents_ql_alpha:\n",
    "    #print(agent)\n",
    "    if agent.param > 0.3:\n",
    "        df = pd.DataFrame(agent.ep_rewards_table)\n",
    "\n",
    "\n",
    "        # Plot the mean rewards as a main line\n",
    "        sns.lineplot(data=df, x='ep', y='avg_'+str(agent.param), label='Alpha '+str(agent.param))\n",
    "\n",
    "        # Fill the area between (mean - std) and (mean + std) as the interval\n",
    "        #plt.fill_between(df['ep'], df['avg_'+str(agent.param)] - df['std_'+str(agent.param)], df['avg_'+str(agent.param)] + df['std_'+str(agent.param)], alpha=0.3)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Mean Reward with Confidence Interval (Window Size = 500 Episodes)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean Reward')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
